# Step 1: Collect the data
# Assume that you have collected the data and stored it in a CSV file called 'crop_data.csv'

# Step 2: Preprocess the data
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
# Load the data into a Pandas dataframe
crop_data = pd.read_csv('yield.csv')
# Drop any rows with missing values
crop_data = crop_data.dropna()
# Split the data into features (X) and target (y)
X = crop_data[['Pesticide_use', 'Avg_rainfall', 'Avg_temperature','Area_used']]
y = crop_data['Crop_yield']

# Scale the features to have zero mean and unit variance
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 5: Choose a machine learning algorithm
from sklearn.ensemble import RandomForestRegressor

# Create a Random Forest Regression model
model = RandomForestRegressor(n_estimators=100, random_state=42)

# Step 6: Train the model
model.fit(X_train, y_train)

# Step 7: Evaluate the model
from sklearn.metrics import mean_squared_error, r2_score

# Make predictions on the testing set
y_pred = model.predict(X_test)

# Calculate the mean squared error and R-squared
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")

# Step 8: Use the model for prediction
# Assume that you have new data with values for pesticide use, average rainfall, and average temperature
new_data = pd.DataFrame({'Pesticide_use': [2.5], 'Avg_rainfall': [100], 'Avg_temperature': [30],'Area_used' : [100] })

# Scale the new data using the same scaler used for training the model
new_data_scaled = scaler.transform(new_data)

# Use the trained model to predict the yield for the new data
yield_pred = model.predict(new_data_scaled)
model.save("model1.h5")
print(f"Predicted yield: {yield_pred}")
